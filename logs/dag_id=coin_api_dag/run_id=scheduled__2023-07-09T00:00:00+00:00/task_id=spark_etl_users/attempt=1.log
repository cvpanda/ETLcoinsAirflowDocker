[2023-07-10T18:39:09.226+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:39:09.241+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:39:09.242+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-10T18:39:09.265+0000] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): spark_etl_users> on 2023-07-09 00:00:00+00:00
[2023-07-10T18:39:09.273+0000] {standard_task_runner.py:57} INFO - Started process 441 to run task
[2023-07-10T18:39:09.278+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'coin_api_dag', 'spark_etl_users', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '25', '--raw', '--subdir', 'DAGS_FOLDER/coin_api.py', '--cfg-path', '/tmp/tmpj4wcm1v8']
[2023-07-10T18:39:09.281+0000] {standard_task_runner.py:85} INFO - Job 25: Subtask spark_etl_users
[2023-07-10T18:39:09.399+0000] {task_command.py:410} INFO - Running <TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [running]> on host 123bc44f0e2f
[2023-07-10T18:39:09.580+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='nahuelCasa' AIRFLOW_CTX_DAG_ID='coin_api_dag' AIRFLOW_CTX_TASK_ID='spark_etl_users' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-10T18:39:09.598+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-10T18:39:09.602+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py
[2023-07-10T18:39:09.817+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-10T18:39:14.740+0000] {spark_submit.py:490} INFO - Traceback (most recent call last):
[2023-07-10T18:39:14.740+0000] {spark_submit.py:490} INFO - File "/opt/***/scripts/apicalls.py", line 172, in <module>
[2023-07-10T18:39:14.741+0000] {spark_submit.py:490} INFO - coin_api = CoinApi()
[2023-07-10T18:39:14.742+0000] {spark_submit.py:490} INFO - File "/opt/***/scripts/apicalls.py", line 21, in __init__
[2023-07-10T18:39:14.742+0000] {spark_submit.py:490} INFO - self.url = config['API_URL']
[2023-07-10T18:39:14.743+0000] {spark_submit.py:490} INFO - KeyError: 'API_URL'
[2023-07-10T18:39:14.834+0000] {spark_submit.py:490} INFO - 23/07/10 18:39:14 INFO ShutdownHookManager: Shutdown hook called
[2023-07-10T18:39:14.840+0000] {spark_submit.py:490} INFO - 23/07/10 18:39:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-02610499-69de-4997-8f8f-6e1f5c1d8449
[2023-07-10T18:39:14.954+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py. Error code is: 1.
[2023-07-10T18:39:14.965+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=coin_api_dag, task_id=spark_etl_users, execution_date=20230709T000000, start_date=20230710T183909, end_date=20230710T183914
[2023-07-10T18:39:14.999+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 25 for task spark_etl_users (Cannot execute: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py. Error code is: 1.; 441)
[2023-07-10T18:39:15.058+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-10T18:39:15.124+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-10T18:49:48.479+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:49:48.495+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:49:48.495+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-10T18:49:48.519+0000] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): spark_etl_users> on 2023-07-09 00:00:00+00:00
[2023-07-10T18:49:48.528+0000] {standard_task_runner.py:57} INFO - Started process 946 to run task
[2023-07-10T18:49:48.536+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'coin_api_dag', 'spark_etl_users', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/coin_api.py', '--cfg-path', '/tmp/tmpp017k44k']
[2023-07-10T18:49:48.540+0000] {standard_task_runner.py:85} INFO - Job 30: Subtask spark_etl_users
[2023-07-10T18:49:48.620+0000] {task_command.py:410} INFO - Running <TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [running]> on host 123bc44f0e2f
[2023-07-10T18:49:48.776+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='nahuelCasa' AIRFLOW_CTX_DAG_ID='coin_api_dag' AIRFLOW_CTX_TASK_ID='spark_etl_users' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-10T18:49:48.797+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-10T18:49:48.799+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py
[2023-07-10T18:49:48.944+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-10T18:49:51.935+0000] {spark_submit.py:490} INFO - Traceback (most recent call last):
[2023-07-10T18:49:51.936+0000] {spark_submit.py:490} INFO - File "/opt/***/scripts/apicalls.py", line 187, in <module>
[2023-07-10T18:49:51.937+0000] {spark_submit.py:490} INFO - main()
[2023-07-10T18:49:51.938+0000] {spark_submit.py:490} INFO - File "/opt/***/scripts/apicalls.py", line 175, in main
[2023-07-10T18:49:51.940+0000] {spark_submit.py:490} INFO - api = CoinApi()
[2023-07-10T18:49:51.940+0000] {spark_submit.py:490} INFO - File "/opt/***/scripts/apicalls.py", line 22, in __init__
[2023-07-10T18:49:51.941+0000] {spark_submit.py:490} INFO - self.url = config['API_URL']
[2023-07-10T18:49:51.941+0000] {spark_submit.py:490} INFO - KeyError: 'API_URL'
[2023-07-10T18:49:52.032+0000] {spark_submit.py:490} INFO - 23/07/10 18:49:52 INFO ShutdownHookManager: Shutdown hook called
[2023-07-10T18:49:52.037+0000] {spark_submit.py:490} INFO - 23/07/10 18:49:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-e835b31b-b08e-4536-8771-ccd69fd6f5bd
[2023-07-10T18:49:52.129+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 422, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py. Error code is: 1.
[2023-07-10T18:49:52.136+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=coin_api_dag, task_id=spark_etl_users, execution_date=20230709T000000, start_date=20230710T184948, end_date=20230710T184952
[2023-07-10T18:49:52.153+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 30 for task spark_etl_users (Cannot execute: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py. Error code is: 1.; 946)
[2023-07-10T18:49:52.196+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-10T18:49:52.228+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-10T18:55:34.668+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:55:34.683+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [queued]>
[2023-07-10T18:55:34.684+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-10T18:55:34.708+0000] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): spark_etl_users> on 2023-07-09 00:00:00+00:00
[2023-07-10T18:55:34.715+0000] {standard_task_runner.py:57} INFO - Started process 209 to run task
[2023-07-10T18:55:34.722+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'coin_api_dag', 'spark_etl_users', 'scheduled__2023-07-09T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/coin_api.py', '--cfg-path', '/tmp/tmpnkvp0v93']
[2023-07-10T18:55:34.725+0000] {standard_task_runner.py:85} INFO - Job 33: Subtask spark_etl_users
[2023-07-10T18:55:34.819+0000] {task_command.py:410} INFO - Running <TaskInstance: coin_api_dag.spark_etl_users scheduled__2023-07-09T00:00:00+00:00 [running]> on host 123bc44f0e2f
[2023-07-10T18:55:34.972+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='nahuelCasa' AIRFLOW_CTX_DAG_ID='coin_api_dag' AIRFLOW_CTX_TASK_ID='spark_etl_users' AIRFLOW_CTX_EXECUTION_DATE='2023-07-09T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-09T00:00:00+00:00'
[2023-07-10T18:55:34.992+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-07-10T18:55:34.994+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master spark://spark:7077 --driver-class-path /tmp/drivers/postgresql-42.5.2.jar --name arrow-spark --queue default /opt/***/scripts/apicalls.py
[2023-07-10T18:55:35.136+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-10T18:55:38.442+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SparkContext: Running Spark version 3.4.1
[2023-07-10T18:55:38.537+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-10T18:55:38.659+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceUtils: ==============================================================
[2023-07-10T18:55:38.660+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-07-10T18:55:38.661+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceUtils: ==============================================================
[2023-07-10T18:55:38.662+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SparkContext: Submitted application: coinRank
[2023-07-10T18:55:38.689+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-07-10T18:55:38.704+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceProfile: Limiting resource is cpu
[2023-07-10T18:55:38.705+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-07-10T18:55:38.773+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SecurityManager: Changing view acls to: ***
[2023-07-10T18:55:38.773+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SecurityManager: Changing modify acls to: ***
[2023-07-10T18:55:38.774+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SecurityManager: Changing view acls groups to:
[2023-07-10T18:55:38.775+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SecurityManager: Changing modify acls groups to:
[2023-07-10T18:55:38.775+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2023-07-10T18:55:39.046+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Utils: Successfully started service 'sparkDriver' on port 43201.
[2023-07-10T18:55:39.094+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO SparkEnv: Registering MapOutputTracker
[2023-07-10T18:55:39.152+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO SparkEnv: Registering BlockManagerMaster
[2023-07-10T18:55:39.179+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-07-10T18:55:39.180+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-07-10T18:55:39.189+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-07-10T18:55:39.217+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6ad17169-a2cf-4e68-b4f0-8f765f857d25
[2023-07-10T18:55:39.239+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-07-10T18:55:39.255+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-07-10T18:55:39.493+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2023-07-10T18:55:39.572+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-07-10T18:55:39.759+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO SparkContext: Added JAR /tmp/drivers/postgresql-42.5.2.jar at spark://123bc44f0e2f:43201/jars/postgresql-42.5.2.jar with timestamp 1689015338426
[2023-07-10T18:55:39.888+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Executor: Starting executor ID driver on host 123bc44f0e2f
[2023-07-10T18:55:39.897+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-07-10T18:55:39.912+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Executor: Fetching spark://123bc44f0e2f:43201/jars/postgresql-42.5.2.jar with timestamp 1689015338426
[2023-07-10T18:55:39.975+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO TransportClientFactory: Successfully created connection to 123bc44f0e2f/192.168.80.5:43201 after 33 ms (0 ms spent in bootstraps)
[2023-07-10T18:55:39.985+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:39 INFO Utils: Fetching spark://123bc44f0e2f:43201/jars/postgresql-42.5.2.jar to /tmp/spark-421762cc-2863-4db7-97f8-8493f8b681a4/userFiles-a50cfee0-8a0e-4df0-afce-3269646315a7/fetchFileTemp10290347367310518493.tmp
[2023-07-10T18:55:40.054+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO Executor: Adding file:/tmp/spark-421762cc-2863-4db7-97f8-8493f8b681a4/userFiles-a50cfee0-8a0e-4df0-afce-3269646315a7/postgresql-42.5.2.jar to class loader
[2023-07-10T18:55:40.065+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45941.
[2023-07-10T18:55:40.066+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO NettyBlockTransferService: Server created on 123bc44f0e2f:45941
[2023-07-10T18:55:40.068+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-07-10T18:55:40.075+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 123bc44f0e2f, 45941, None)
[2023-07-10T18:55:40.082+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO BlockManagerMasterEndpoint: Registering block manager 123bc44f0e2f:45941 with 434.4 MiB RAM, BlockManagerId(driver, 123bc44f0e2f, 45941, None)
[2023-07-10T18:55:40.088+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 123bc44f0e2f, 45941, None)
[2023-07-10T18:55:40.089+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 123bc44f0e2f, 45941, None)
[2023-07-10T18:55:40.407+0000] {spark_submit.py:490} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2023-07-10T18:55:40.408+0000] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2023-07-10T18:55:40.613+0000] {spark_submit.py:490} INFO - Iniciando..
[2023-07-10T18:55:40.614+0000] {spark_submit.py:490} INFO - Conectando a DB
[2023-07-10T18:55:44.315+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-07-10T18:55:44.324+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:44 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2023-07-10T18:55:46.748+0000] {spark_submit.py:490} INFO - root
[2023-07-10T18:55:46.749+0000] {spark_submit.py:490} INFO - |-- timestamp: string (nullable = false)
[2023-07-10T18:55:46.750+0000] {spark_submit.py:490} INFO - |-- uuid: string (nullable = false)
[2023-07-10T18:55:46.750+0000] {spark_submit.py:490} INFO - |-- symbol: string (nullable = true)
[2023-07-10T18:55:46.751+0000] {spark_submit.py:490} INFO - |-- name: string (nullable = false)
[2023-07-10T18:55:46.751+0000] {spark_submit.py:490} INFO - |-- color: string (nullable = true)
[2023-07-10T18:55:46.752+0000] {spark_submit.py:490} INFO - |-- iconUrl: string (nullable = true)
[2023-07-10T18:55:46.752+0000] {spark_submit.py:490} INFO - |-- marketCap: string (nullable = false)
[2023-07-10T18:55:46.752+0000] {spark_submit.py:490} INFO - |-- price: string (nullable = false)
[2023-07-10T18:55:46.753+0000] {spark_submit.py:490} INFO - |-- listedAt: string (nullable = false)
[2023-07-10T18:55:46.753+0000] {spark_submit.py:490} INFO - |-- tier: string (nullable = false)
[2023-07-10T18:55:46.753+0000] {spark_submit.py:490} INFO - |-- change: string (nullable = false)
[2023-07-10T18:55:46.753+0000] {spark_submit.py:490} INFO - |-- rank: string (nullable = false)
[2023-07-10T18:55:46.754+0000] {spark_submit.py:490} INFO - 
[2023-07-10T18:55:46.754+0000] {spark_submit.py:490} INFO - delete duplicates from dataframe
[2023-07-10T18:55:46.770+0000] {spark_submit.py:490} INFO - adding transaction fee column
[2023-07-10T18:55:46.878+0000] {spark_submit.py:490} INFO - root
[2023-07-10T18:55:46.879+0000] {spark_submit.py:490} INFO - |-- timestamp: string (nullable = false)
[2023-07-10T18:55:46.880+0000] {spark_submit.py:490} INFO - |-- uuid: string (nullable = false)
[2023-07-10T18:55:46.882+0000] {spark_submit.py:490} INFO - |-- symbol: string (nullable = true)
[2023-07-10T18:55:46.882+0000] {spark_submit.py:490} INFO - |-- name: string (nullable = false)
[2023-07-10T18:55:46.883+0000] {spark_submit.py:490} INFO - |-- color: string (nullable = true)
[2023-07-10T18:55:46.885+0000] {spark_submit.py:490} INFO - |-- iconUrl: string (nullable = true)
[2023-07-10T18:55:46.886+0000] {spark_submit.py:490} INFO - |-- marketCap: string (nullable = false)
[2023-07-10T18:55:46.886+0000] {spark_submit.py:490} INFO - |-- price: string (nullable = false)
[2023-07-10T18:55:46.887+0000] {spark_submit.py:490} INFO - |-- listedAt: string (nullable = false)
[2023-07-10T18:55:46.887+0000] {spark_submit.py:490} INFO - |-- tier: string (nullable = false)
[2023-07-10T18:55:46.887+0000] {spark_submit.py:490} INFO - |-- change: string (nullable = false)
[2023-07-10T18:55:46.888+0000] {spark_submit.py:490} INFO - |-- rank: string (nullable = false)
[2023-07-10T18:55:46.888+0000] {spark_submit.py:490} INFO - |-- transactionFee: decimal(10,2) (nullable = true)
[2023-07-10T18:55:46.889+0000] {spark_submit.py:490} INFO - 
[2023-07-10T18:55:47.556+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:47 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2023-07-10T18:55:47.841+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:47 INFO CodeGenerator: Code generated in 229.587111 ms
[2023-07-10T18:55:48.021+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Registering RDD 6 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2023-07-10T18:55:48.037+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Got map stage job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-10T18:55:48.038+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2023-07-10T18:55:48.038+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Parents of final stage: List()
[2023-07-10T18:55:48.042+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Missing parents: List()
[2023-07-10T18:55:48.048+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-10T18:55:48.148+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 26.8 KiB, free 434.4 MiB)
[2023-07-10T18:55:48.212+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.4 MiB)
[2023-07-10T18:55:48.217+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 123bc44f0e2f:45941 (size: 12.0 KiB, free: 434.4 MiB)
[2023-07-10T18:55:48.227+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2023-07-10T18:55:48.243+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-10T18:55:48.245+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-07-10T18:55:48.307+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (123bc44f0e2f, executor driver, partition 0, PROCESS_LOCAL, 15906 bytes)
[2023-07-10T18:55:48.326+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-07-10T18:55:49.556+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO CodeGenerator: Code generated in 50.392035 ms
[2023-07-10T18:55:49.590+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO CodeGenerator: Code generated in 9.478908 ms
[2023-07-10T18:55:49.625+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO CodeGenerator: Code generated in 20.673836 ms
[2023-07-10T18:55:49.655+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO PythonRunner: Times: total = 980, boot = 964, init = 15, finish = 1
[2023-07-10T18:55:49.780+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2925 bytes result sent to driver
[2023-07-10T18:55:49.809+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1512 ms on 123bc44f0e2f (executor driver) (1/1)
[2023-07-10T18:55:49.822+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 34703
[2023-07-10T18:55:49.824+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-07-10T18:55:49.834+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO DAGScheduler: ShuffleMapStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.764 s
[2023-07-10T18:55:49.836+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO DAGScheduler: looking for newly runnable stages
[2023-07-10T18:55:49.837+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO DAGScheduler: running: Set()
[2023-07-10T18:55:49.838+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO DAGScheduler: waiting: Set()
[2023-07-10T18:55:49.839+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO DAGScheduler: failed: Set()
[2023-07-10T18:55:49.881+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-07-10T18:55:49.917+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2023-07-10T18:55:49.993+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:49 INFO CodeGenerator: Code generated in 57.617808 ms
[2023-07-10T18:55:50.065+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2023-07-10T18:55:50.072+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-10T18:55:50.073+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
[2023-07-10T18:55:50.076+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2023-07-10T18:55:50.077+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Missing parents: List()
[2023-07-10T18:55:50.082+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-10T18:55:50.114+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 46.6 KiB, free 434.3 MiB)
[2023-07-10T18:55:50.134+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 434.3 MiB)
[2023-07-10T18:55:50.140+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 123bc44f0e2f:45941 (size: 20.8 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.140+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2023-07-10T18:55:50.141+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-10T18:55:50.143+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-07-10T18:55:50.159+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (123bc44f0e2f, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-07-10T18:55:50.171+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2023-07-10T18:55:50.219+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 123bc44f0e2f:45941 in memory (size: 12.0 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.288+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO ShuffleBlockFetcherIterator: Getting 1 (15.8 KiB) non-empty blocks including 1 (15.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-07-10T18:55:50.291+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
[2023-07-10T18:55:50.355+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 8963 bytes result sent to driver
[2023-07-10T18:55:50.360+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 204 ms on 123bc44f0e2f (executor driver) (1/1)
[2023-07-10T18:55:50.361+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-07-10T18:55:50.367+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.261 s
[2023-07-10T18:55:50.370+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-10T18:55:50.371+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-07-10T18:55:50.374+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.308748 s
[2023-07-10T18:55:50.449+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO CodeGenerator: Code generated in 26.875538 ms
[2023-07-10T18:55:50.468+0000] {spark_submit.py:490} INFO - +-------------------+-------------+------+--------------------+-------+--------------------+-----------+--------------------+----------+----+------+----+--------------+
[2023-07-10T18:55:50.469+0000] {spark_submit.py:490} INFO - |          timestamp|         uuid|symbol|                name|  color|             iconUrl|  marketCap|               price|  listedAt|tier|change|rank|transactionFee|
[2023-07-10T18:55:50.469+0000] {spark_submit.py:490} INFO - +-------------------+-------------+------+--------------------+-------+--------------------+-----------+--------------------+----------+----+------+----+--------------+
[2023-07-10T18:55:50.469+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|D7B1x_ks7WhV5|   LTC|            Litecoin|#345d9d|https://cdn.coinr...| 6814568683|    96.1622668791199|1382572800|   1| -0.75|  13|          9.62|
[2023-07-10T18:55:50.470+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    xz24e0BjL|  SHIB|           Shiba Inu|#fda32b|https://cdn.coinr...| 4480684582|0.000007600484154272|1620650373|   1| -1.24|  18|          0.00|
[2023-07-10T18:55:50.471+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    _H5FVG9iW|   UNI|             Uniswap|#ff007a|https://cdn.coinr...| 4253732075|   5.294959400756395|1600323371|   1|  0.76|  21|          0.53|
[2023-07-10T18:55:50.472+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|qFakph2rpuMOL|   MKR|               Maker|#1abc9c|https://cdn.coinr...|  911563509|   932.3525051903364|1502236800|   1| -4.86|  48|         93.24|
[2023-07-10T18:55:50.472+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|FEbS54wxo4oIl|   VET|             VeChain|#4bc0fa|https://cdn.coinr...| 1237585365|0.018537621678085023|1533427200|   1| -0.96|  37|          0.00|
[2023-07-10T18:55:50.473+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|-l8Mn2pVlRs-p|   XRP|                 XRP|#000000|https://cdn.coinr...|24765524288| 0.47394241610030735|1421798400|   1|  0.87|   6|          0.05|
[2023-07-10T18:55:50.474+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    ncYFcP709|  CAKE|         PancakeSwap|#fe9555|https://cdn.coinr...| 1942739571|  1.5200228758855607|1613642379|   1|  4.36|  30|          0.15|
[2023-07-10T18:55:50.474+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    ymQub4fuB|   FIL|            Filecoin|#0090ff|https://cdn.coinr...| 1890164755|   4.348406420392508|1602839473|   1| -0.07|  31|          0.44|
[2023-07-10T18:55:50.475+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    aMNLwaUbY|   ICP|Internet Computer...|#00042b|https://cdn.coinr...| 1783785535|   4.074629351577711|1601555742|   1|  0.55|  32|          0.41|
[2023-07-10T18:55:50.476+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    1Uo6s62Oc|   ARB|            ARBITRUM|#9fc5e8|https://cdn.coinr...| 1446425923|  1.1344517044377258|1679022695|   1|  0.25|  34|          0.11|
[2023-07-10T18:55:50.477+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    25W7FG7om|   DOT|            Polkadot|#E6007A|https://cdn.coinr...| 6428814768|   5.156587349231153|1598365200|   1|  1.53|  14|          0.52|
[2023-07-10T18:55:50.477+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|vSo2fu9iE1s0Y|  BUSD|         Binance USD|#f0b90b|https://cdn.coinr...| 4243267071|  0.9996848819665143|1563197940|   1| -0.08|  22|          0.10|
[2023-07-10T18:55:50.477+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|HIVsRcGKkPFtW|  USDT|          Tether USD|#22a079|https://cdn.coinr...|83319897429|  0.9995103152120275|1420761600|   1| -0.11|   3|          0.10|
[2023-07-10T18:55:50.477+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42| QUC5kVAxSoB-|    MX|            MX Token|   null|https://cdn.coinr...| 1202212779|   2.763168764160871|1568732160|   1| -0.29|  38|          0.28|
[2023-07-10T18:55:50.477+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    Z96jIvLU7|   IMX|         Immutable X|#000000|https://cdn.coinr...| 1405439010|   0.702719506381894|1649387294|   1| -0.18|  35|          0.07|
[2023-07-10T18:55:50.478+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    pxtKbG5rg|  SAND|         The Sandbox|#00adef|https://cdn.coinr...| 1021151894| 0.41834132438420074|1613583024|   1|  0.32|  43|          0.04|
[2023-07-10T18:55:50.478+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    dvUj0CzDZ|  AVAX|           Avalanche|#e84242|https://cdn.coinr...| 4664109703|  13.491826807084127|1600961596|   1| -1.94|  17|          1.35|
[2023-07-10T18:55:50.478+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|Knsels4_Ol-Ny|  ATOM|              Cosmos|#5064fb|https://cdn.coinr...| 3064299343|   9.311219748245474|1552520100|   1| -2.68|  24|          0.93|
[2023-07-10T18:55:50.478+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|aKzUVe4Hh_CON|  USDC|                USDC|#7894b4|https://cdn.coinr...|27315333188|   0.999389656914236|1539043200|   1| -0.10|   5|          0.10|
[2023-07-10T18:55:50.478+0000] {spark_submit.py:490} INFO - |2023-07-10 18:55:42|    ixgUfzmLR|  AAVE|                Aave|#B6509E|https://cdn.coinr...| 1034689245|   70.53964804790614|1603447311|   1| -3.65|  40|          7.05|
[2023-07-10T18:55:50.479+0000] {spark_submit.py:490} INFO - +-------------------+-------------+------+--------------------+-------+--------------------+-----------+--------------------+----------+----+------+----+--------------+
[2023-07-10T18:55:50.479+0000] {spark_submit.py:490} INFO - only showing top 20 rows
[2023-07-10T18:55:50.479+0000] {spark_submit.py:490} INFO - 
[2023-07-10T18:55:50.523+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2023-07-10T18:55:50.545+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Registering RDD 11 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2023-07-10T18:55:50.546+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-07-10T18:55:50.552+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0)
[2023-07-10T18:55:50.552+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Parents of final stage: List()
[2023-07-10T18:55:50.557+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Missing parents: List()
[2023-07-10T18:55:50.562+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-10T18:55:50.570+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 26.8 KiB, free 434.3 MiB)
[2023-07-10T18:55:50.583+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 123bc44f0e2f:45941 in memory (size: 20.8 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.596+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.0 KiB, free 434.4 MiB)
[2023-07-10T18:55:50.597+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 123bc44f0e2f:45941 (size: 12.0 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.601+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2023-07-10T18:55:50.602+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-10T18:55:50.605+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2023-07-10T18:55:50.606+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (123bc44f0e2f, executor driver, partition 0, PROCESS_LOCAL, 15906 bytes)
[2023-07-10T18:55:50.610+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2023-07-10T18:55:50.679+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO PythonRunner: Times: total = 18, boot = -1146, init = 1164, finish = 0
[2023-07-10T18:55:50.726+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2925 bytes result sent to driver
[2023-07-10T18:55:50.730+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 124 ms on 123bc44f0e2f (executor driver) (1/1)
[2023-07-10T18:55:50.730+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2023-07-10T18:55:50.733+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.168 s
[2023-07-10T18:55:50.734+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: looking for newly runnable stages
[2023-07-10T18:55:50.734+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: running: Set()
[2023-07-10T18:55:50.735+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: waiting: Set()
[2023-07-10T18:55:50.735+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: failed: Set()
[2023-07-10T18:55:50.747+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2023-07-10T18:55:50.755+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2023-07-10T18:55:50.828+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO CodeGenerator: Code generated in 48.856545 ms
[2023-07-10T18:55:50.882+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Got job 3 (toLocalIterator at /opt/***/scripts/apicalls.py:105) with 1 output partitions
[2023-07-10T18:55:50.883+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Final stage: ResultStage 5 (toLocalIterator at /opt/***/scripts/apicalls.py:105)
[2023-07-10T18:55:50.884+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2023-07-10T18:55:50.885+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Missing parents: List()
[2023-07-10T18:55:50.892+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-07-10T18:55:50.913+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 50.0 KiB, free 434.3 MiB)
[2023-07-10T18:55:50.931+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.0 KiB, free 434.3 MiB)
[2023-07-10T18:55:50.931+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 123bc44f0e2f:45941 (size: 22.0 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.932+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2023-07-10T18:55:50.932+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[16] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-07-10T18:55:50.933+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2023-07-10T18:55:50.938+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 123bc44f0e2f:45941 in memory (size: 12.0 KiB, free: 434.4 MiB)
[2023-07-10T18:55:50.941+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (123bc44f0e2f, executor driver, partition 0, NODE_LOCAL, 7363 bytes)
[2023-07-10T18:55:50.942+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)
[2023-07-10T18:55:50.995+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO ShuffleBlockFetcherIterator: Getting 1 (15.8 KiB) non-empty blocks including 1 (15.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2023-07-10T18:55:50.996+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2023-07-10T18:55:51.083+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 24883 bytes result sent to driver
[2023-07-10T18:55:51.088+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 149 ms on 123bc44f0e2f (executor driver) (1/1)
[2023-07-10T18:55:51.089+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2023-07-10T18:55:51.093+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO DAGScheduler: ResultStage 5 (toLocalIterator at /opt/***/scripts/apicalls.py:105) finished in 0.192 s
[2023-07-10T18:55:51.095+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-07-10T18:55:51.095+0000] {spark_submit.py:490} INFO - 23/07/10 18:55:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2023-07-10T18:56:04.063+0000] {spark_submit.py:490} INFO - Conexi√≥n a DB Cerrada
[2023-07-10T18:56:04.064+0000] {spark_submit.py:490} INFO - Se inserto correctamente
[2023-07-10T18:56:04.209+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO SparkContext: Invoking stop() from shutdown hook
[2023-07-10T18:56:04.209+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2023-07-10T18:56:04.228+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO SparkUI: Stopped Spark web UI at http://123bc44f0e2f:4040
[2023-07-10T18:56:04.260+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-07-10T18:56:04.277+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO MemoryStore: MemoryStore cleared
[2023-07-10T18:56:04.278+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO BlockManager: BlockManager stopped
[2023-07-10T18:56:04.284+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-07-10T18:56:04.288+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-07-10T18:56:04.299+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO SparkContext: Successfully stopped SparkContext
[2023-07-10T18:56:04.299+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO ShutdownHookManager: Shutdown hook called
[2023-07-10T18:56:04.300+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-421762cc-2863-4db7-97f8-8493f8b681a4/pyspark-ccc7616d-4663-45b2-be05-93cad150ffd3
[2023-07-10T18:56:04.309+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f071eff-ec5c-4f9a-8276-4417740208a4
[2023-07-10T18:56:04.315+0000] {spark_submit.py:490} INFO - 23/07/10 18:56:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-421762cc-2863-4db7-97f8-8493f8b681a4
[2023-07-10T18:56:04.478+0000] {taskinstance.py:1350} INFO - Marking task as SUCCESS. dag_id=coin_api_dag, task_id=spark_etl_users, execution_date=20230709T000000, start_date=20230710T185534, end_date=20230710T185604
[2023-07-10T18:56:04.540+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 0
[2023-07-10T18:56:04.578+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
